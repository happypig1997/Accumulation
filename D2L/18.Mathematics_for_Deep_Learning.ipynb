{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.11. Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.11.1. Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude Shannon 1948\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Information* can be encoded in anything with a particular sequence of one or more encoding formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.11.1.1. Self-information\n",
    "\n",
    "A formula connects possiblity to information quantity(bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since information embodies the abstract possibility of an event, how do we map the possibility to the number of bits? Shannon introduced the terminology bit as the unit of information, which was originally created by John Tukey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of binary digits of length  ùëõ  contains  ùëõ  bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Probablility of one even happen (encoding with $n$ bits) is $\\frac{1}{2^n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, can we generalize to a math function which can transfer the probability  ùëù  to the number of bits? Shannon gave the answer by defining self-information\n",
    " $I(X) = - \\log_2 (p)\n",
    " $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nansum(x):\n",
    "    # Define nansum, as pytorch doesn't offer it inbuilt.\n",
    "    return x[~torch.isnan(x)].sum()\n",
    "\n",
    "def self_information(p):\n",
    "    return -torch.log2(torch.tensor(p)).item()\n",
    "\n",
    "self_information(1 / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.11.2. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.11.2.1. Motivating Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The information we gain by observing a random variable does not depend on what we call the elements, or the presence of additional elements which have probability zero.\n",
    "\n",
    "2. The information we gain by observing two random variables is no more than the sum of the information we gain by observing them separately. If they are independent, then it is exactly the sum.\n",
    "\n",
    "3. The information gained when observing (nearly) certain events is (nearly) zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.11.2.2. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(X) = - E_{x \\sim P} [I(X)] = - E_{x \\sim P} [\\log p(x)]\n",
    "$\n",
    "\n",
    "Negative Estimation * Self Entropy\n",
    "\n",
    "$P$: p.d.f. or p.m.f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $X$ is discrete,\n",
    "$H(X) = - \\sum_i p_i \\log p_i \\text{, where } p_i = P(X_i)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if $X$ is continuous, we also refer entropy as differential entropy,\n",
    "$H(X) = - \\int_x p(x) \\log p(x) \\; dx\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6855)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    entropy = - p * torch.log2(p)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = nansum(entropy)\n",
    "    return out\n",
    "\n",
    "entropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.11.2.3. Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\log$\n",
    "- Negative\n",
    "- Expectation Function multiply Self Entropy\n",
    "  - We can interpret $H(S)$ as the average expectation of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.11.2.4. Properties of Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H(X)\\geq 0$ for all discrete $X$(entropy can be negative for continuous $X$)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3329e0d83167662813ee7576243cbad107dc88fc06bdc797c0439a814c2ed5ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
